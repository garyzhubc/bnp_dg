\documentclass{article}
\usepackage[autonum]{tchdr}
\usepackage{amsmath, amsfonts, graphicx, float, color, tikz, listings, amsthm}
\usetikzlibrary{bayesnet}
\title{On the geometry of  Dirichlet process mixture models}
\newcommand{\tre}{\cdots}
\newcommand{\Qexp}{\mcQ^\text{exp}}
\newcommand{\Qfull}{\mcQ^\text{full}}
\newcommand{\kgibbs}{K^\text{Gibbs}}
\newcommand{\kbgibbs}{K^\text{blockGibbs}}
\newcommand{\ytest}{y_\text{test}}
\newcommand{\ytrain}{y_\text{train}}
\newcommand{\xtest}{x_\text{test}}
\newcommand{\xtrain}{x_\text{train}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\makeatletter
\newcommand*\dashline{\rotatebox[origin=c]{90}{$\dabar@\dabar@\dabar@$}}
\author{Boyan Beronov, Christian Weilbach, Peiyuan Zhu}
\begin{document}
	\maketitle
	\section{Background}
	Generative model with Dirichlet process with concentration parameter $\alpha$ and base distribution $H$ as prior and likelihood function $f$ can be written as follows:
	\[
		\Theta&=\sum_{k=1}^\infty\theta_k\delta_{\psi_k}\dist\distDP\left(\alpha,H\right)\cr
		Y_n&\dist f\left(\cdot|\Theta\right)
	\]
	Define manifold $\mcM=\left\{m(y,\Theta)=f(y|\Theta)P(\Theta):\Theta=\sum_k\theta_k\delta_{\psi_k},\theta\in\triangle\right\}$ where $\triangle$ is an infinite-dimensional simplex and $f$ is the mixture density: $f(y|\theta,\psi)=\sum_{k=1}^\infty\theta_kf(y|\psi_k)$ and $P(\Theta)=\distGEM(d\theta)H(\psi)$ where GEM stands for  Griffiths, Engen and McCloske stick-breaking distribution \cite{pitman02}. Define information metric on this manifold as the Kullback-Leibler divergence. A truncation method defines path $\gamma(t)$ for $K=\lfloor t\rfloor$ between two densities $m,m'$ in which $\theta=(\theta_1,\theta_2,\dots,\theta_K,0,0,\cdots)$. The truncation introduced by \cite{rasmussen00} defines the path having approximating prior $(\theta_1,\cdots,\theta_K)\dist\distDir\left(\frac{\alpha}{K},\cdots,\frac{\alpha}{K}\right)$ whereas the truncation introduced by \cite{pitman02} defines a path by applying change of coordinate transformation $(\theta_1,\cdots,\theta_K)=T(\beta_1,\cdots,\beta_K)=(\beta_1,(1-\beta_1)\beta_2,\cdots,(1-\beta_1)\beta_K)$ in which independent distribution $\beta_K\distiid\distBeta(1,\alpha)$ was obtained by neutrality. 
	
	This framework works similarly for the normalized completely random measure models. Previous work synthesized truncation of normalized completely random measures as TFAs \cite{campbell19} and IFAs  by \cite{nguyen20} for the exponential families scoped by \cite{broderick18}. Generative model with normalized completely random measure with rate measure $\nu$ and base distribution $H$ as prior and likelihood function $f$ can be written as follows:
	\[
		\Theta&=\sum_{k=1}^\infty\theta_k\delta_{\psi_k}\dist\distNCRM\left(\nu,H\right)\cr
		Y_n&\dist f\left(\cdot|\Theta\right)
	\]
	In our formalization, IFAs define path ??? and TFAs define path ???
	{
		\small
		\bibliographystyle{unsrtnat}
		\bibliography{sources}
	}
\end{document}