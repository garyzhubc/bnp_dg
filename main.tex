\documentclass[letterpaper]{article}
\usepackage{uai2020}
\usepackage[margin=1in]{geometry}

\usepackage[sort&compress,numbers]{natbib}

\usepackage{times}

\usepackage[autonum]{tchdr}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\newcommand{\tre}{\cdots}
\newcommand{\Qexp}{\mcQ^\text{exp}}
\newcommand{\Qfull}{\mcQ^\text{full}}
\newcommand{\kgibbs}{K^\text{Gibbs}}
\newcommand{\kbgibbs}{K^\text{blockGibbs}}
\newcommand{\todo}{\textcolor{red}{\text{continue???}}}


\title{On Truncation of  Completely Random Measures}

\author{ 
{\{\bf{Boyan Beronov} \quad \bf {Christian Weilbach}\} \qquad \bf {Peiyuan Zhu}}  \\
\qquad\qquad Department of Computer Science  \qquad Department of Statistics \\
University of British Columbia\\
Vancouver, BC V6T 1Z4 \\
}

\begin{document}

\maketitle
\section{Introduction}

\section{Finite truncation of completely random measures}
The process of a data generation from a mixture model with completely random measure prior can be described as follows. First, we draw a completely random measure from rate measure $\nu_\beta$ parameterized by $D$-dimensional vector $\beta$ and base distribution $H$. Then for each atom $(\theta_k,\psi_k)\in\preals\times\Psi $, we draw an independent (usually discrete) random variable $X_n$ from likelihood $h$ that takes zero value with non-zero probability. Usually we need $\int\min(1,\theta)\nu_\beta(d\theta)<\infty$  so that the total weight is finite $\sum_{k=1}^\infty\theta_k<\infty$ and $\nu_\beta(\preals)<\infty$ so that there are infinitely many atoms generated. Denote $\Theta\dist\distNCRM$ as drawing $\Xi\dist\distCRM(\nu,H)$ then normalize $\Theta=\frac{\Xi}{\Xi(\Psi)}$. We can put this generative process into a graphical model as follows:
\[
	&\Theta=\sum_k\theta_k\delta_{\psi_k}\dist\distNCRM(\nu_\beta,H)\cr
	&X_n|\Theta\dist\distLP(h,\Theta)\cr
	&Y_n|X_n\distind f(\cdot|X_n)\label{eqn:full}
\]

Now we consider a generative model that unifies the notation of  \cite{nguyen20,campbell19} by encoding the fixed truncation level $K$ into the generative model:
\[
	&\Theta=\sum_{k=1}^\infty\theta_k\delta_{\psi_k}\dist\distNCRM(\nu_\beta,H)\cr
	&\Theta'=\sum_{k=1}^K\theta_k\delta_{\psi_k}\cr
	&X_n|\Theta'\dist\distLP(h,\Theta')\cr
	&Y_n|X_n\distind f(\cdot|X_n)\label{eqn:trunc}
\]

Let $\beta_0$ be the parameter of the completely random measure that we want to simulate. Both truncation methods provided error bounds in the total variation norm between the marginal likelihood $p_{\beta(K),K}$ and $p_{\beta_0,\infty}$ as:
\[
	\sup_A\left|p_{\beta_K,K}(A)-p_{\beta_0,\infty}(A)\right|\le B(K,\beta_0)
\]
for some function $B(K,\beta_0)$. The independent finite representation synthesized by \cite{nguyen20} uses $\nu_{\beta(K,\beta_0)}$ from a generalized exponential family \cite{broderick18} with additional discount parameter to independently generate $\theta_1,\theta_2,\cdots,\theta_K\distiid\nu_{\beta(K,\beta_0)}(\cdot)$ and $\beta(K)$ is a parameter choice for truncation level  $K$. The series representation synthesized by \cite{campbell19} uses the exact $\nu_{\beta(K,\beta_0)}=\nu$ but conditionally generate auxiliary variables $\xi_k|\xi_{k-1}$ for a given ordering of the atoms and set $\theta_k=\kappa(\xi_k)$ with a transformation $\kappa$. The superposition representation synthesized by \cite{campbell19} simulates independent $\theta_{k,1},\theta_{k,2},\cdots,\theta_{k,C_k}\distiid\nu_{\beta(K,\beta_0)}$ for a given ordering of the sub-measures so the generative model becomes $\Theta'=\sum_{k=1}^K\sum_{i=1}^{C_k}\theta_{ki}\delta_{\psi_{ki}}$ in \ref{eqn:trunc}. This is inefficient in inference \cite{zhu20,nguyen20} so we do not consider it in our framework here. 

\section{The information geometry of truncation}

Define the manifold $\mcM_{(\beta,k)\in\reals^D\times\nats}$ as the space of  joint distributions of $Y,X,\Theta$ induced by rate measure $\nu_\beta$ and truncation level $K$ with chart $\mcM\rightarrow\reals^D\times\nats$.  For a given $K$, the manifold $\mcM_{\beta\in\reals^D}$ is naturally endowed with Fisher information metric:
\[
	G(\beta)=\int\grad_\beta\log p_\beta(y)\grad_\beta\log p_\beta(y)^Tp_\beta(y)dy
\]

For any differentiable curve $\gamma:[0,1]\rightarrow\mcM_k$ this metric defines a notion of path length:
\[
	L(\gamma)=\int_0^1\sqrt{\frac{d\gamma(t)^T}{dt}G(\gamma(t))\frac{d\gamma(t)}{dt}}dt
\]



{
\small
\bibliographystyle{unsrtnat}
\bibliography{sources}
}

\end{document}
\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
