\documentclass[letterpaper]{article}
\usepackage{uai2020}
\usepackage[margin=1in]{geometry}

\usepackage[sort&compress,numbers]{natbib}

\usepackage{times}

\usepackage[autonum]{tchdr}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\newcommand{\tre}{\cdots}
\newcommand{\Qexp}{\mcQ^\text{exp}}
\newcommand{\Qfull}{\mcQ^\text{full}}
\newcommand{\kgibbs}{K^\text{Gibbs}}
\newcommand{\kbgibbs}{K^\text{blockGibbs}}
\newcommand{\todo}{\textcolor{red}{\text{continue???}}}


\title{Truncated Random Measures from a Information Geometric View}

\author{ 
	{\{\bf{Boyan Beronov} \quad \bf {Christian Weilbach}\} \qquad \bf {Peiyuan Zhu}}  \\
	\qquad\qquad Department of Computer Science  \qquad Department of Statistics \\
	University of British Columbia\\
	Vancouver, BC V6T 1Z4 \\
}

\begin{document}
	
	\maketitle
	\section{Introduction}
	
	\section{Finite truncation of completely random measures}
	The process of a data generation from a mixture model with completely random measure prior can be described as follows. First, we draw a completely random measure from rate measure $\nu_\beta$ parameterized by $D$-dimensional vector $\beta$ and base distribution $H$. Then for each atom $(\theta_k,\psi_k)\in\preals\times\Psi $, we draw an independent (usually discrete) random variable $X_n$ from likelihood $h$ that takes zero value with non-zero probability. Usually we need $\int\min(1,\theta)\nu_\beta(d\theta)<\infty$  so that the total weight is finite $\sum_{k=1}^\infty\theta_k<\infty$ and $\nu_\beta(\preals)<\infty$ so that there are infinitely many atoms generated. Denote $\Theta\dist\distNCRM$ as drawing $\Xi\dist\distCRM(\nu,H)$ then normalize $\Theta=\frac{\Xi}{\Xi(\Psi)}$. We can put this generative process into a graphical model as follows:
	\[
	&\Theta=\sum_k\theta_k\delta_{\psi_k}\dist\distNCRM(\nu_\beta,H)\cr
	&X_n|\Theta\dist\distLP(h,\Theta)\cr
	&Y_n|X_n\distind f(\cdot|X_n)\label{eqn:full}
	\]
	
	Now we consider a generative model that unifies the notation of  \cite{nguyen20,campbell19} by encoding the fixed truncation level $K$ into the generative model:
	\[
	&\Theta=\sum_{k=1}^\infty\theta_k\delta_{\psi_k}\dist\distNCRM(\nu_\beta,H)\cr
	&\Theta_K=\sum_{k=1}^K\theta_k\delta_{\psi_k}\cr
	&X_n|\Theta'\dist\distLP(h,\Theta_K)\cr
	&Y_n|X_n\distind f(\cdot|X_n)\label{eqn:trunc}
	\]
	
	Let $\beta_0$ be the parameter of the completely random measure that we want to simulate. Both \cite{campbell19,nguyen20} provided error bounds in the total variation norm between the marginal likelihood $p_{\beta(K,\beta_0),K}$ and $p_{\beta_0,\infty}$ as:
	\[
	\sup_A\left|p_{\beta(K,\beta_0),K}(A)-p_{\beta_0,\infty}(A)\right|\le B(K,\beta_0)
	\]
	for some function $B(K,\beta_0)$ and smooth parametrization function $\beta(K,\beta_0)$. For \cite{campbell19} the function $b$ equals:
	\[
	B(K,\beta_0)=1-C(K,\beta_0)^K
	\]
	for deterministic function $C$; \cite{nguyen20} the function $B$ equals:
	\[
	B(K,\beta_0)=\frac{C_1(\beta_0)+C_2(\beta_0)\ln^2N+C_3(\beta_0)\ln N\ln K}{K}.
	\]
	for constants $C_1,C_2,C_3$ depending on $\beta_0$. \cite{nguyen20} also provided a lower bound on the truncation error. The independent finite representation synthesized by \cite{nguyen20} uses $\nu_{\beta(K,\beta_0)}$ from the exponential family \cite{broderick18} with mass parameter $\gamma'$, log-partition function $A(\theta)$, and natural parameter $\mu(\theta)$ 
	\[
	\nu_{\beta(K,\beta_0)}=\gamma'\beta_0^{-1}\exp\left(\left<\beta_0,\left(\mu(\theta),-A(\theta)\right)^T\right>\right)
	\]
	to independently generate $\theta_1,\theta_2,\cdots,\theta_K\distiid\nu_{\beta(K,\beta_0)}(\cdot)$.  For mass parameter 
	\[
	c=\gamma'\exp\left(\left<\beta_0,\left(\mu(\theta),-A(\theta)\right)^T\right>\right)
	\] and the normalizing constant:
	\[
	S(\xi,\beta_0)=\int\theta^\xi\exp\left(\left<\beta_0,\left(\mu(\theta),-A(\theta)\right)^T\right>\right)d\theta,
	\] 
	the truncated rate measure $\nu(K,\beta_0)$ equals:
	\[
	\nu(K,\beta_0)=\frac{\theta^{\frac{c}{K}-1}\exp\left(\left<\beta_0,\left(\mu(\theta),-A(\theta)\right)^T\right>\right)}{Z\left(\frac{c}{K}-1,\beta_0\right)}
	\] 
	
	The series representation synthesized by \cite{campbell19} uses the exact rate measure $\nu_{\beta(K,\beta_0)}=\nu_{\beta_0}$ but conditionally generate auxiliary variables $\xi_k|\xi_{k-1}$ for a given ordering of the atoms and set $\theta_k=\tau(\xi_k)$ with a transformation $\tau$. In particular for series representation $\xi_k=(V_k,\Gamma_k)$ where $V_k$ follows independent marking distribution $V_k\distiid G$ and $\Gamma_k$ is the unit-rate Poisson process on the positive real line $\Gamma_k=\sum_{l=1}^kE_l,E_l\distiid\distExp(1)$. The superposition representation synthesized by \cite{campbell19} simulates independent $\theta_{k,1},\theta_{k,2},\cdots,\theta_{k,C_k}\distiid\nu_{\beta(K,\beta_0)}$ for a given ordering of the sub-measures so  that $\nu_{\beta_0}=\sum_{k=1}^\infty C_{\beta(k,\beta_0)}\nu_k$ for some deterministic function $C$. Therefore the generative model becomes $\Theta_K=\sum_{k=1}^K\sum_{i=1}^{C_k}\theta_{ki}\delta_{\psi_{ki}}$ in \ref{eqn:trunc}. This method of truncation is inefficient in inference \cite{zhu20,nguyen20} so we do not consider it in our framework here. 
	
	\section{The information geometry of truncation}
	
	Define the manifold $M_{\beta\in\reals^D,k}$ as the space of  marginal likelihood induced by rate measure $\nu_\beta$ and truncation level $k$ each equipped with with chart $\mcM_k\rightarrow\reals^D$. We let $\mcM=\{(M_{\beta\in\reals^D,k},k):k\in\nats\}$ as union of the manifolds for all truncation levels. For a truncation level $K$, the Riemannian manifold $\mcM_{\beta\in\reals^D,K}$ is naturally endowed with the Fisher information metric:
	\[
	G_K(\beta)=\int\grad_\beta\log p_{\beta,K}(y)\grad_\beta\log p_{\beta,K}(y)^Tp_{\beta,K}(y)dy
	\]
	
	For any differentiable curve $\gamma:[0,1]\rightarrow\mcM_K$ this metric defines a notion of path length:
	\[
	L_K(\gamma)=\int_0^1\sqrt{\frac{d\gamma(t)^T}{dt}G_K(\gamma(t))\frac{d\gamma(t)}{dt}}dt
	\]
	
	
	
	{
		\small
		\bibliographystyle{unsrtnat}
		\bibliography{sources}
	}
	
\end{document}
\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}